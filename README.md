
---

# ğŸš€ NASA APOD ETL Pipeline (Airflow + Postgres)

This project is a fully automated **ETL data pipeline** built using **Apache Airflow 2.9+**, **PostgreSQL**, and the public **NASA Astronomy Picture of the Day (APOD) API**.

The pipeline fetches NASAâ€™s daily image metadata, processes it, and stores it in a Postgres database for analytics and downstream use.

---

## ğŸ“Œ **Project Overview**

The goal of this pipeline is simple:

1. **Extract**
   Fetch latest APOD data using NASA API.

2. **Transform**
   Parse JSON response and clean required fields.

3. **Load**
   Store final dataset into a PostgreSQL table.

The pipeline runs **every day** using Airflowâ€™s modern scheduling system (`schedule="@daily"`).

---

## ğŸ— **Tech Stack**

* **Apache Airflow 2.9+**
* **PostgreSQL**
* **Docker / Docker Compose**
* **Airflow Providers**

  * `apache-airflow-providers-http`
  * `apache-airflow-providers-postgres`
* **Python 3.12**
* **Requests library**

---

## ğŸ“‚ **Project Structure**

```
.
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ **Airflow DAG Summary**

### **DAG ID:** `nasa_apod_postgres`

### **Schedule:** Daily (`@daily`)

### **Catchup:** Disabled

### **Tasks:**

| Task               | Description                       |
| ------------------ | --------------------------------- |
| **extract_apod**   | Calls NASA API (HTTP GET)         |
| **transform_apod** | Cleans and prepares JSON fields   |
| **load_apod**      | Inserts cleaned row into Postgres |

---

## ğŸ”Œ **Environment Variables Required**

Add these in `.env` or Airflow UI â†’ Variables:

```
NASA_API_KEY=your_api_key_here
POSTGRES_CONN_ID=postgres_default
```

---

## ğŸ“¦ **Installation & Setup**

### 1ï¸âƒ£ Clone repository

```bash
git clone https://github.com/your-username/nasa-apod-airflow.git
cd nasa-apod-airflow
```

### 2ï¸âƒ£ Install dependencies

```
pip install -r requirements.txt
```

### 3ï¸âƒ£ Start Airflow (Docker)

```bash
docker compose up -d
```

### 4ï¸âƒ£ Access Airflow UI

Go to:
**[http://localhost:8080](http://localhost:8080)**

Enable the DAG â†’ watch the ETL pipeline run.

---

## ğŸ—„ **PostgreSQL Table Schema**

```
CREATE TABLE nasa_apod (
    date DATE PRIMARY KEY,
    title TEXT,
    explanation TEXT,
    media_type TEXT,
    url TEXT
);
```

---

## ğŸ“Š Output Example

| date       | title       | media_type | url                                                 |
| ---------- | ----------- | ---------- | --------------------------------------------------- |
| 2025-01-02 | Galaxy View | image      | [https://apod.nasa.gov/](https://apod.nasa.gov/)... |

---

## ğŸ¯ **Why This Project Is Valuable**

âœ” Real ETL workflow
âœ” Scheduling + automation
âœ” API â†’ Airflow â†’ Postgres pipeline
âœ” Industry-standard stack
âœ” Recruiter-friendly, perfect for Data Engineer intern roles

---

## ğŸ¤ **Contributions**

Feel free to fork this project and improve the transformations, add logging, or build Dash/Streamlit dashboards on top of the dataset.

---

## ğŸ“œ License

MIT License

---


Just tell me and Iâ€™ll add it!
